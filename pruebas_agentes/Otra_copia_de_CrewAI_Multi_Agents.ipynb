{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CrewAI Multi-Agents Demo\n",
        "\n",
        "# Installs"
      ],
      "metadata": {
        "id": "KlFmjIkxZcpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q crewai==0.28.8\n",
        "!pip install -q crewai_tools==0.1.6\n",
        "!pip install -q langchain_community==0.0.29\n",
        "!pip install -q arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvJa9OPSbovr",
        "outputId": "42e63e52-2e21-4d6d-8d1f-26003a541857",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m871.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.7/160.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/342.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.1.20 requires langchain-community<0.1,>=0.0.38, but you have langchain-community 0.0.29 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "5PQIQhK8Z2Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "NuxhN3l48W_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew\n",
        "from crewai_tools import BaseTool\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import arxiv\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "lEcd2r0LZoP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext dotenv\n",
        "%dotenv /content/drive/MyDrive/.env"
      ],
      "metadata": {
        "id": "Cj-0i1n0TXJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "xQb-EEIYTbe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define API tools"
      ],
      "metadata": {
        "id": "m2LyxcfObH8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FetchPapersTool(BaseTool):\n",
        "    name: str = \"Fetch AI Research Papers\"\n",
        "    description: str = \"Fetch latest AI research papers from Arxiv based on a search query and optional author affiliations.\"\n",
        "\n",
        "    def _run(self, keywords: list=[\"artificial intelligence\"], affiliations: list = [], max_results=5) -> list:\n",
        "        query = \" OR \".join(keywords)  # Convert list into an OR-separated query\n",
        "        if affiliations:\n",
        "            affiliation_query = \" OR \".join([f\"au:{aff}\" for aff in affiliations])\n",
        "            query = f\"({query}) AND ({affiliation_query})\"\n",
        "\n",
        "        search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
        "        papers = []\n",
        "        for result in search.results():\n",
        "            papers.append({\"title\": result.title, \"summary\": result.summary, \"url\": result.entry_id})\n",
        "        return papers\n",
        "\n",
        "fetch_papers_tool = FetchPapersTool()"
      ],
      "metadata": {
        "id": "FpicTVK-X33c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Agents"
      ],
      "metadata": {
        "id": "zTseSPT2bemJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name='gpt-4o', api_key = OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "ppU_JNBZuqIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Research Agent"
      ],
      "metadata": {
        "id": "JC1gVbotbhnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_agent = Agent(\n",
        "    role = \"Paper Retrieval Agent\",\n",
        "    goal = \"Retrieve the latest research papers from Arxiv on a given set of topics.\",\n",
        "    backstory = (\n",
        "        \"Uses Arxiv to explore the most recent and relevant papers \"\n",
        "        \"given a set of keywords. This is a very up-to-date repository, \"\n",
        "        \"which guarantees that we are accessing the cutting-edge contributions to the specified fields.\"\n",
        "    ),\n",
        "    tools=[fetch_papers_tool],\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "9u1XCX0ybpaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7cf887b2-756e-47bc-967f-18178670ae40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Summarization Agent"
      ],
      "metadata": {
        "id": "qOybVgj9bsW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_agent = Agent(\n",
        "    role=\"Summarization Agent\",\n",
        "    goal=\"Summarize research papers in layman's terms.\",\n",
        "    backstory = (\n",
        "        \"Digests the contents of the papers, no matter how technically complex. \"\n",
        "        \"Extracts the key ideas so that they can be understood by someone interested in the field \"\n",
        "        \"but not necessarily trained on the subject.\"\n",
        "    ),\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "l67T3CiQb4Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. *Influencer* Agent"
      ],
      "metadata": {
        "id": "DHm6orRvb5vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_generation_agent = Agent(\n",
        "    role = \"Content Generation Agent\",\n",
        "    goal = \"Creates LinkedIn posts and newsletter articles from research summaries.\",\n",
        "    backstory = (\n",
        "        \"Showing super-powers to engage readers, this influencer agent \"\n",
        "        \"takes texts on scientific or technological subjects and builds \"\n",
        "        \"engaging summaries in a newsletter format with a potential high impact.\"\n",
        "    ),\n",
        "    llm = llm\n",
        ")"
      ],
      "metadata": {
        "id": "xIuyth6Jb_vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Tasks"
      ],
      "metadata": {
        "id": "WzC3XFxEcCUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Research Task"
      ],
      "metadata": {
        "id": "nfYcYRMkcFIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_task = Task(\n",
        "    description=\"Fetch the latest research papers on the topics provided, \"\n",
        "                \"focusing on the work of researchers \"\n",
        "                \"from the institutions specified.\",\n",
        "    agent=retrieval_agent,\n",
        "    expected_output= \"A list of texts containing the contents of the most recent \"\n",
        "                     \"and relevant papers on the topics specified.\",\n",
        "    input_variables=[\"keywords\", \"affiliations\"]\n",
        ")"
      ],
      "metadata": {
        "id": "jsQSkOTGcK62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Summarization Task"
      ],
      "metadata": {
        "id": "U8mBogm-cf7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_task = Task(\n",
        "    description=\"Summarize the key findings of the retrieved research\",\n",
        "    agent=summarization_agent,\n",
        "    expected_output = (\n",
        "        \"A list of summaries distilling the contents of the \"\n",
        "        \"technical articles in the input, 500 to 1000 words each.\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "zq7rsWz3cpFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Content generation Task"
      ],
      "metadata": {
        "id": "Rt2jTkz7cqQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_task = Task(\n",
        "    description=\"Generate an engaging newsletter\",\n",
        "    agent=content_generation_agent,\n",
        "    expected_output = \"A 500 to 1000 word newsletter in Markdown format based on the provided summaries. \"\n",
        "                      \"Written in an accessible and engaging language, it must visually appealing by using \"\n",
        "                      \"bullets, bold letters, italic letters and hyperlinks to the sources.\",\n",
        ")"
      ],
      "metadata": {
        "id": "v9aI9LvCc11Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestrate Agents"
      ],
      "metadata": {
        "id": "g6Us-hNsda3k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFrPpP_AY5tz"
      },
      "outputs": [],
      "source": [
        "crew = Crew(\n",
        "    agents=[retrieval_agent, summarization_agent, content_generation_agent],\n",
        "    tasks=[retrieval_task, summarization_task, content_task],\n",
        "    verbose = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute"
      ],
      "metadata": {
        "id": "Sv8tdk1tezvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = [\"artificial intelligence\", \"deep learning\", \"machine learning\", \"LLMs\", \"neural networks\", \"agentic ai\"]\n",
        "affiliations = [\"openai\", \"deepmind\", \"stanford\", \"microsoft\"]\n",
        "results = crew.kickoff(inputs={'keywords': keywords, 'affiliations': affiliations})"
      ],
      "metadata": {
        "id": "9DfmRfn5e0J8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0d5166-e047-42d0-ff8e-dd20051a1acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Paper Retrieval Agent\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Fetch the latest research papers on the topics provided, focusing on the work of researchers from the institutions specified.\u001b[00m\n",
            "\u001b[95m \n",
            "\n",
            "[{'title': 'L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling', 'summary': \"We rigorously establish a bipartite mutual information scaling law in natural\\nlanguage that governs long-range dependencies. This scaling law, which we show\\nis distinct from and scales independently of the conventional two-point mutual\\ninformation, is the key to understanding long-context language modeling. Using\\nthis scaling law, we formulate the Long-context Language Modeling (L$^2$M)\\ncondition, which relates a model's capacity for effective long context length\\nmodeling to the scaling of its latent state size for storing past information.\\nOur results are validated through experiments on both transformers and state\\nspace models. This work establishes a theoretical foundation that guides the\\ndevelopment of large language models toward longer context lengths.\", 'url': 'http://arxiv.org/abs/2503.04725v1'}, {'title': 'Enough Coin Flips Can Make LLMs Act Bayesian', 'summary': 'Large language models (LLMs) exhibit the ability to generalize given few-shot\\nexamples in their input prompt, an emergent capability known as in-context\\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\\nreasoning in ways that are consistent with a Bayesian framework or rely on\\npattern matching. Using a controlled setting of biased coin flips, we find\\nthat: (1) LLMs often possess biased priors, causing initial divergence in\\nzero-shot settings, (2) in-context evidence outweighs explicit bias\\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\\ndeviations primarily due to miscalibrated priors rather than flawed updates,\\nand (4) attention magnitude has negligible effect on Bayesian inference. With\\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\\npriors in a Bayesian manner.', 'url': 'http://arxiv.org/abs/2503.04722v1'}, {'title': 'Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation', 'summary': 'Scene flow estimation is a foundational task for many robotic applications,\\nincluding robust dynamic object detection, automatic labeling, and sensor\\nsynchronization. Two types of approaches to the problem have evolved: 1)\\nSupervised and 2) optimization-based methods. Supervised methods are fast\\nduring inference and achieve high-quality results, however, they are limited by\\nthe need for large amounts of labeled training data and are susceptible to\\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\\nface the problem of domain gaps but usually suffer from substantial runtime,\\nexhibit artifacts, or fail to converge to the right solution. In this work, we\\nmitigate several limitations of existing optimization-based methods. To this\\nend, we 1) introduce a simple voxel grid-based model that improves over the\\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\\nmultiframe loss formulation. 3) We combine both contributions in our new\\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\\nby EulerFlow among unsupervised methods while achieving comparable performance\\nat a fraction of the computational cost. Floxels achieves a massive speedup of\\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\\nachieves a speedup of ~14x.', 'url': 'http://arxiv.org/abs/2503.04718v1'}, {'title': 'MIGHTEE: exploring the relationship between spectral index, redshift and radio luminosity', 'summary': 'It has been known for many years that there is an apparent trend for the\\nspectral index ({\\\\alpha}) of radio sources to steepen with redshift z, which\\nhas led to attempts to select high-redshift objects by searching for radio\\nsources with steep spectra. In this study we use data from the MeerKAT, LOFAR,\\nGMRT, and uGMRT telescopes, particularly using the MIGHTEE and superMIGHTEE\\nsurveys, to select compact sources over a wide range of redshifts and\\nluminosities. We investigate the relationship between spectral index,\\nluminosity and redshift and compare our results to those of previous studies.\\nAlthough there is a correlation between {\\\\alpha} and z in our sample for some\\ncombinations of frequency where good data are available, there is a clear\\noffset between the {\\\\alpha}-z relations in our sample and those derived\\npreviously from samples of more luminous objects; in other words, the\\n{\\\\alpha}-z relation is different for low and high luminosity sources. The\\nrelationships between {\\\\alpha} and luminosity are also weak in our sample but\\nin general the most luminous sources are steeper-spectrum and this trend is\\nextended by samples from previous studies. In detail, we argue that both a\\n{\\\\alpha}-luminosity relation and an {\\\\alpha}-z relation can be found in the\\ndata, but it is the former that drives the apparent {\\\\alpha}-z relation\\nobserved in earlier work, which only appears because of the strong\\nredshift-luminosity relation in bright, flux density-limited samples.\\nSteep-spectrum selection should be applied with caution in searching for high-z\\nsources in future deep surveys.', 'url': 'http://arxiv.org/abs/2503.04717v1'}, {'title': 'Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining', 'summary': 'The impressive capabilities of Large Language Models (LLMs) across diverse\\ntasks are now well-established, yet their effective deployment necessitates\\ncareful hyperparameter optimization. Through extensive empirical studies\\ninvolving grid searches across diverse configurations, we discover universal\\nscaling laws governing these hyperparameters: optimal learning rate follows a\\npower-law relationship with both model parameters and data sizes, while optimal\\nbatch size scales primarily with data sizes. Our analysis reveals a convex\\noptimization landscape for hyperparameters under fixed models and data size\\nconditions. This convexity implies an optimal hyperparameter plateau. We\\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\\ncommunity. Its estimated values on the test set are merely 0.07\\\\% away from the\\nglobally optimal LLM performance found via an exhaustive search. These laws\\ndemonstrate remarkable robustness across variations in model sparsity, training\\ndata distribution, and model shape. To our best known, this is the first work\\nthat unifies different model shapes and structures, such as Mixture-of-Experts\\nmodels and dense transformers, as well as establishes optimal hyperparameter\\nscaling laws across diverse data distributions. This exhaustive optimization\\nprocess demands substantial computational resources, utilizing nearly one\\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\\ntotal. To facilitate reproducibility and further research, we will\\nprogressively release all loss measurements and model checkpoints through our\\ndesignated repository https://step-law.github.io/', 'url': 'http://arxiv.org/abs/2503.04715v1'}]\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Paper Retrieval Agent] Task output: 1. **Title:** L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling\n",
            "   - **Summary:** \"We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L$^2$M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.\"\n",
            "   - [Read more](http://arxiv.org/abs/2503.04725v1)\n",
            "\n",
            "2. **Title:** Enough Coin Flips Can Make LLMs Act Bayesian\n",
            "   - **Summary:** \"Large language models (LLMs) exhibit the ability to generalize given few-shot examples in their input prompt, an emergent capability known as in-context learning (ICL). We investigate whether LLMs utilize ICL to perform structured reasoning in ways that are consistent with a Bayesian framework or rely on pattern matching. Using a controlled setting of biased coin flips, we find that: (1) LLMs often possess biased priors, causing initial divergence in zero-shot settings, (2) in-context evidence outweighs explicit bias instructions, (3) LLMs broadly follow Bayesian posterior updates, with deviations primarily due to miscalibrated priors rather than flawed updates, and (4) attention magnitude has negligible effect on Bayesian inference. With sufficient demonstrations of biased coin flips via ICL, LLMs update their priors in a Bayesian manner.\"\n",
            "   - [Read more](http://arxiv.org/abs/2503.04722v1)\n",
            "\n",
            "3. **Title:** Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation\n",
            "   - **Summary:** \"Scene flow estimation is a foundational task for many robotic applications, including robust dynamic object detection, automatic labeling, and sensor synchronization. Two types of approaches to the problem have evolved: 1) Supervised and 2) optimization-based methods. Supervised methods are fast during inference and achieve high-quality results, however, they are limited by the need for large amounts of labeled training data and are susceptible to domain gaps. In contrast, unsupervised test-time optimization methods do not face the problem of domain gaps but usually suffer from substantial runtime, exhibit artifacts, or fail to converge to the right solution. In this work, we mitigate several limitations of existing optimization-based methods. To this end, we 1) introduce a simple voxel grid-based model that improves over the standard MLP-based formulation in multiple dimensions and 2) introduce a new multiframe loss formulation. 3) We combine both contributions in our new method, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only by EulerFlow among unsupervised methods while achieving comparable performance at a fraction of the computational cost. Floxels achieves a massive speedup of more than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10 minutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels achieves a speedup of ~14x.\"\n",
            "   - [Read more](http://arxiv.org/abs/2503.04718v1)\n",
            "\n",
            "4. **Title:** MIGHTEE: exploring the relationship between spectral index, redshift and radio luminosity\n",
            "   - **Summary:** \"It has been known for many years that there is an apparent trend for the spectral index ({\\\\alpha}) of radio sources to steepen with redshift z, which has led to attempts to select high-redshift objects by searching for radio sources with steep spectra. In this study we use data from the MeerKAT, LOFAR, GMRT, and uGMRT telescopes, particularly using the MIGHTEE and superMIGHTEE surveys, to select compact sources over a wide range of redshifts and luminosities. We investigate the relationship between spectral index, luminosity and redshift and compare our results to those of previous studies. Although there is a correlation between {\\\\alpha} and z in our sample for some combinations of frequency where good data are available, there is a clear offset between the {\\\\alpha}-z relations in our sample and those derived previously from samples of more luminous objects; in other words, the {\\\\alpha}-z relation is different for low and high luminosity sources. The relationships between {\\\\alpha} and luminosity are also weak in our sample but in general the most luminous sources are steeper-spectrum and this trend is extended by samples from previous studies. In detail, we argue that both a {\\\\alpha}-luminosity relation and an {\\\\alpha}-z relation can be found in the data, but it is the former that drives the apparent {\\\\alpha}-z relation observed in earlier work, which only appears because of the strong redshift-luminosity relation in bright, flux density-limited samples. Steep-spectrum selection should be applied with caution in searching for high-z sources in future deep surveys.\"\n",
            "   - [Read more](http://arxiv.org/abs/2503.04717v1)\n",
            "\n",
            "5. **Title:** Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining\n",
            "   - **Summary:** \"The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization. Through extensive empirical studies involving grid searches across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch size scales primarily with data sizes. Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions. This convexity implies an optimal hyperparameter plateau. We contribute a universal, plug-and-play optimal hyperparameter tool for the community. Its estimated values on the test set are merely 0.07\\\\% away from the globally optimal LLM performance found via an exhaustive search. These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape. To our best known, this is the first work that unifies different model shapes and structures, such as Mixture-of-Experts models and dense transformers, as well as establishes optimal hyperparameter scaling laws across diverse data distributions. This exhaustive optimization process demands substantial computational resources, utilizing nearly one million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and hyperparameters from scratch and consuming approximately 100 trillion tokens in total. To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository https://step-law.github.io/\"\n",
            "   - [Read more](http://arxiv.org/abs/2503.04715v1)\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Summarization Agent\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Summarize the key findings of the retrieved research\u001b[00m\n",
            "\u001b[95m \n",
            "\n",
            "1. **L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling**  \n",
            "   This paper introduces a novel scaling law for mutual information that is crucial to understanding long-context language modeling. Unlike the conventional two-point mutual information, this bipartite mutual information scaling law operates independently. It leads to the formulation of the Long-context Language Modeling (L$^2$M) condition, which explains how a model's ability to handle long context lengths correlates with the scaling of its latent state size for storing past information. The theoretical findings are supported by experiments with transformers and state space models, which provide a framework for developing large language models capable of managing extended context lengths. [Read more](http://arxiv.org/abs/2503.04725v1)\n",
            "\n",
            "2. **Enough Coin Flips Can Make LLMs Act Bayesian**  \n",
            "   This study delves into the ability of Large Language Models (LLMs) to generalize from few-shot examples using in-context learning (ICL), exploring whether this process aligns with a Bayesian framework. Through analyzing biased coin flip scenarios, the research reveals that LLMs exhibit initially biased priors but can update these in a Bayesian manner with sufficient in-context examples. The study finds that in-context evidence often overrides explicit bias instructions, and attention magnitude does not significantly impact Bayesian inference. The research highlights the Bayesian-like behavior of LLMs, with deviations largely due to miscalibrated priors. [Read more](http://arxiv.org/abs/2503.04722v1)\n",
            "\n",
            "3. **Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation**  \n",
            "   This research addresses the foundational task of scene flow estimation, pivotal for robotics applications such as dynamic object detection and sensor synchronization. The paper critiques existing methods, highlighting limitations like domain gaps in supervised approaches and inefficiency in unsupervised ones. It presents a voxel grid-based model and a new multiframe loss formulation, culminating in the Floxels method. Floxels outperforms existing unsupervised methods in speed and competitiveness, offering significant runtime reductions while maintaining high performance on benchmarks like Argoverse 2. [Read more](http://arxiv.org/abs/2503.04718v1)\n",
            "\n",
            "4. **MIGHTEE: exploring the relationship between spectral index, redshift and radio luminosity**  \n",
            "   This study examines the correlation between spectral index, redshift, and radio luminosity using data from various telescopes and surveys. It challenges previous assumptions by showing that the spectral index-redshift relationship differs for low and high luminosity sources. The paper argues that the observed {\\alpha}-z relation in earlier studies is driven by the {\\alpha}-luminosity relationship, influenced by the redshift-luminosity connection in bright samples. The research advises caution when using steep-spectrum selection for identifying high-redshift sources in future surveys. [Read more](http://arxiv.org/abs/2503.04717v1)\n",
            "\n",
            "5. **Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining**  \n",
            "   This paper identifies universal scaling laws for hyperparameters in LLM pretraining, enhancing model deployment efficiency. It finds that optimal learning rates follow a power-law relationship with model parameters and data sizes, while batch size scales primarily with data sizes. The research uncovers a convex optimization landscape, suggesting an optimal hyperparameter plateau. A plug-and-play tool for optimal hyperparameter estimation is introduced, with performance estimates closely aligning with global optima. This work unifies different model structures and establishes robust scaling laws across varied data distributions, significantly contributing to efficient LLM training. [Read more](http://arxiv.org/abs/2503.04715v1)\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Summarization Agent] Task output: 1. **L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling**  \n",
            "   This paper introduces a novel scaling law for mutual information that is crucial to understanding long-context language modeling. Unlike the conventional two-point mutual information, this bipartite mutual information scaling law operates independently. It leads to the formulation of the Long-context Language Modeling (L$^2$M) condition, which explains how a model's ability to handle long context lengths correlates with the scaling of its latent state size for storing past information. The theoretical findings are supported by experiments with transformers and state space models, which provide a framework for developing large language models capable of managing extended context lengths. [Read more](http://arxiv.org/abs/2503.04725v1)\n",
            "\n",
            "2. **Enough Coin Flips Can Make LLMs Act Bayesian**  \n",
            "   This study delves into the ability of Large Language Models (LLMs) to generalize from few-shot examples using in-context learning (ICL), exploring whether this process aligns with a Bayesian framework. Through analyzing biased coin flip scenarios, the research reveals that LLMs exhibit initially biased priors but can update these in a Bayesian manner with sufficient in-context examples. The study finds that in-context evidence often overrides explicit bias instructions, and attention magnitude does not significantly impact Bayesian inference. The research highlights the Bayesian-like behavior of LLMs, with deviations largely due to miscalibrated priors. [Read more](http://arxiv.org/abs/2503.04722v1)\n",
            "\n",
            "3. **Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation**  \n",
            "   This research addresses the foundational task of scene flow estimation, pivotal for robotics applications such as dynamic object detection and sensor synchronization. The paper critiques existing methods, highlighting limitations like domain gaps in supervised approaches and inefficiency in unsupervised ones. It presents a voxel grid-based model and a new multiframe loss formulation, culminating in the Floxels method. Floxels outperforms existing unsupervised methods in speed and competitiveness, offering significant runtime reductions while maintaining high performance on benchmarks like Argoverse 2. [Read more](http://arxiv.org/abs/2503.04718v1)\n",
            "\n",
            "4. **MIGHTEE: exploring the relationship between spectral index, redshift and radio luminosity**  \n",
            "   This study examines the correlation between spectral index, redshift, and radio luminosity using data from various telescopes and surveys. It challenges previous assumptions by showing that the spectral index-redshift relationship differs for low and high luminosity sources. The paper argues that the observed {\\alpha}-z relation in earlier studies is driven by the {\\alpha}-luminosity relationship, influenced by the redshift-luminosity connection in bright samples. The research advises caution when using steep-spectrum selection for identifying high-redshift sources in future surveys. [Read more](http://arxiv.org/abs/2503.04717v1)\n",
            "\n",
            "5. **Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining**  \n",
            "   This paper identifies universal scaling laws for hyperparameters in LLM pretraining, enhancing model deployment efficiency. It finds that optimal learning rates follow a power-law relationship with model parameters and data sizes, while batch size scales primarily with data sizes. The research uncovers a convex optimization landscape, suggesting an optimal hyperparameter plateau. A plug-and-play tool for optimal hyperparameter estimation is introduced, with performance estimates closely aligning with global optima. This work unifies different model structures and establishes robust scaling laws across varied data distributions, significantly contributing to efficient LLM training. [Read more](http://arxiv.org/abs/2503.04715v1)\n",
            "\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Content Generation Agent\u001b[00m\n",
            "\u001b[1m\u001b[95m [INFO]: == Starting Task: Generate an engaging newsletter\u001b[00m\n",
            "\u001b[95m \n",
            "\n",
            "1. **L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling** - This paper introduces a new way to think about how language models understand long pieces of text. It focuses on a concept called mutual information, which is a way of measuring the dependency between two variables. The paper suggests a new rule, or scaling law, for this mutual information that helps explain how language models can store and process long sequences of information. The researchers propose the Long-context Language Modeling (L$^2$M) condition, which describes how a model's capacity to manage long texts is linked to the size of its memory for storing past information. They support their theory with experiments using transformers and other models, providing a framework for building large models that can handle extended texts.\n",
            "\n",
            "2. **Enough Coin Flips Can Make LLMs Act Bayesian** - This study explores how Large Language Models (LLMs) can learn from a few examples, similar to how humans make predictions based on limited information. It investigates whether this learning process follows Bayesian principles, where models update their beliefs based on new evidence. By simulating coin flips with different biases, the researchers found that LLMs start with a biased view but can adjust their beliefs in a Bayesian way when given more examples. The study also found that even when LLMs are given explicit instructions that contradict the examples, they often rely more on the examples to make decisions. This behavior suggests that LLMs have a Bayesian-like way of learning, although their initial assumptions may not always be accurate.\n",
            "\n",
            "3. **Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation** - This research focuses on improving how machines understand movement in a scene, which is crucial for robotics tasks like detecting moving objects. The paper critiques current methods for estimating scene flow, pointing out that supervised methods struggle with real-world differences, while unsupervised methods can be slow. The researchers propose a new approach called Floxels, which uses a 3D grid of tiny cubes (voxels) to model the scene. They also introduce a new way of measuring error over multiple frames, which helps the model learn better. Floxels is faster and more competitive than existing unsupervised methods, offering significant reductions in processing time while maintaining high accuracy on tests like the Argoverse 2 benchmark.\n",
            "\n",
            "4. **MIGHTEE: exploring the relationship between spectral index, redshift and radio luminosity** - This study examines how certain properties of galaxies, like how bright they are in radio waves, relate to their distance from us (redshift) and the characteristics of their radio emissions (spectral index). By using data from different telescopes and surveys, the researchers challenge previous beliefs about these relationships. They find that the connection between spectral index and redshift varies depending on how bright the galaxy is. The paper suggests that earlier observations of a simple relationship between these factors were actually influenced by other underlying relationships. The study advises caution when using certain characteristics to identify distant galaxies in future research.\n",
            "\n",
            "5. **Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining** - This paper explores how to efficiently scale the settings, or hyperparameters, used when training large language models. It identifies universal rules for how these hyperparameters should change as the model's size or the amount of data increases. Specifically, it finds that the best learning rates follow a consistent pattern based on the size of the model and data, while the batch size is more related to the data size. The research also suggests there is an optimal range for these settings, making it easier to find the best configuration for training. A tool is introduced to help estimate these optimal settings, showing results that closely match the best possible outcomes. This work helps unify different model structures and provides strong guidelines for training large language models more efficiently.\n",
            "\u001b[00m\n",
            "\u001b[1m\u001b[92m [DEBUG]: == [Content Generation Agent] Task output: # 📰 Cutting-Edge Insights in AI and Robotics\n",
            "\n",
            "Welcome to this week's edition of our newsletter, where we dive into groundbreaking research shaping the future of AI and robotics. From novel scaling laws in language models to innovative approaches in scene flow estimation, let's explore these exciting advancements. \n",
            "\n",
            "## 📚 L$^2$M: A New Perspective on Long-Context Language Modeling\n",
            "\n",
            "Understanding long pieces of text is a complex task for language models. The paper on **L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling** introduces a fresh approach to tackling this challenge. \n",
            "\n",
            "- **Concept**: Focuses on mutual information, a measure of dependency between variables.\n",
            "- **Innovation**: Proposes a new scaling law to explain how models store and process lengthy sequences.\n",
            "- **Outcome**: The Long-context Language Modeling (L$^2$M) condition links a model's capacity to manage extended texts to its latent memory size. \n",
            "- **Support**: Experiments with transformers and other models bolster this framework, paving the way for developing large models that handle long texts effectively.\n",
            "\n",
            "*Explore the full paper [here](http://arxiv.org/abs/2503.04725v1).*\n",
            "\n",
            "## 🎲 Bayesian Behavior in Large Language Models\n",
            "\n",
            "Can Large Language Models (LLMs) learn like humans with limited data? The study **Enough Coin Flips Can Make LLMs Act Bayesian** investigates this intriguing question.\n",
            "\n",
            "- **Focus**: Examines few-shot learning and its alignment with Bayesian principles.\n",
            "- **Findings**: LLMs start with biased views but update beliefs Bayesianly with more examples.\n",
            "- **Insight**: Even against explicit contradictory instructions, LLMs rely more on examples, showcasing a Bayesian-like learning approach.\n",
            "\n",
            "*Read more about this study [here](http://arxiv.org/abs/2503.04722v1).*\n",
            "\n",
            "## 🚀 Floxels: Advancing Scene Flow Estimation\n",
            "\n",
            "The **Floxels** paper presents a leap forward in unsupervised scene flow estimation, crucial for robotics applications.\n",
            "\n",
            "- **Challenge**: Existing methods face domain gaps and inefficiencies.\n",
            "- **Solution**: A voxel grid-based model paired with a new multiframe loss formulation.\n",
            "- **Advantage**: Floxels outperform current unsupervised methods in speed and accuracy, significantly reducing runtime while maintaining high benchmark performance.\n",
            "\n",
            "*Discover the details [here](http://arxiv.org/abs/2503.04718v1).*\n",
            "\n",
            "## 🌌 MIGHTEE: Unveiling Galactic Relationships\n",
            "\n",
            "Exploring the cosmos, the **MIGHTEE** study delves into the relationship between spectral index, redshift, and radio luminosity.\n",
            "\n",
            "- **Research**: Uses data from various telescopes to challenge existing assumptions.\n",
            "- **Revelation**: The spectral index-redshift relationship varies across luminosity levels.\n",
            "- **Caution**: Warns against relying solely on steep-spectrum selection for identifying high-redshift sources.\n",
            "\n",
            "*Learn more about this cosmic exploration [here](http://arxiv.org/abs/2503.04717v1).*\n",
            "\n",
            "## 🔧 Optimizing Hyperparameters in LLM Pretraining\n",
            "\n",
            "The paper **Predictable Scale: Part I** identifies universal scaling laws for hyperparameters in language model pretraining.\n",
            "\n",
            "- **Objective**: Enhance model deployment efficiency through optimal hyperparameter scaling.\n",
            "- **Discovery**: Learning rates follow a power-law relationship with model and data sizes; batch sizes scale with data.\n",
            "- **Tool**: A plug-and-play estimator aids in finding optimal settings, aligning closely with global optima.\n",
            "\n",
            "*Check out the full research [here](http://arxiv.org/abs/2503.04715v1).*\n",
            "\n",
            "---\n",
            "\n",
            "These groundbreaking studies continue to push the boundaries of what is possible in AI and robotics. Stay tuned for more insights and innovations in our next edition!\n",
            "\n",
            "\u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(results))\n"
      ],
      "metadata": {
        "id": "ttNiC_LV1UGu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae2913d1-b454-4e6d-ea04-7b5bffd95f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# 📰 Cutting-Edge Insights in AI and Robotics\n\nWelcome to this week's edition of our newsletter, where we dive into groundbreaking research shaping the future of AI and robotics. From novel scaling laws in language models to innovative approaches in scene flow estimation, let's explore these exciting advancements. \n\n## 📚 L$^2$M: A New Perspective on Long-Context Language Modeling\n\nUnderstanding long pieces of text is a complex task for language models. The paper on **L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling** introduces a fresh approach to tackling this challenge. \n\n- **Concept**: Focuses on mutual information, a measure of dependency between variables.\n- **Innovation**: Proposes a new scaling law to explain how models store and process lengthy sequences.\n- **Outcome**: The Long-context Language Modeling (L$^2$M) condition links a model's capacity to manage extended texts to its latent memory size. \n- **Support**: Experiments with transformers and other models bolster this framework, paving the way for developing large models that handle long texts effectively.\n\n*Explore the full paper [here](http://arxiv.org/abs/2503.04725v1).*\n\n## 🎲 Bayesian Behavior in Large Language Models\n\nCan Large Language Models (LLMs) learn like humans with limited data? The study **Enough Coin Flips Can Make LLMs Act Bayesian** investigates this intriguing question.\n\n- **Focus**: Examines few-shot learning and its alignment with Bayesian principles.\n- **Findings**: LLMs start with biased views but update beliefs Bayesianly with more examples.\n- **Insight**: Even against explicit contradictory instructions, LLMs rely more on examples, showcasing a Bayesian-like learning approach.\n\n*Read more about this study [here](http://arxiv.org/abs/2503.04722v1).*\n\n## 🚀 Floxels: Advancing Scene Flow Estimation\n\nThe **Floxels** paper presents a leap forward in unsupervised scene flow estimation, crucial for robotics applications.\n\n- **Challenge**: Existing methods face domain gaps and inefficiencies.\n- **Solution**: A voxel grid-based model paired with a new multiframe loss formulation.\n- **Advantage**: Floxels outperform current unsupervised methods in speed and accuracy, significantly reducing runtime while maintaining high benchmark performance.\n\n*Discover the details [here](http://arxiv.org/abs/2503.04718v1).*\n\n## 🌌 MIGHTEE: Unveiling Galactic Relationships\n\nExploring the cosmos, the **MIGHTEE** study delves into the relationship between spectral index, redshift, and radio luminosity.\n\n- **Research**: Uses data from various telescopes to challenge existing assumptions.\n- **Revelation**: The spectral index-redshift relationship varies across luminosity levels.\n- **Caution**: Warns against relying solely on steep-spectrum selection for identifying high-redshift sources.\n\n*Learn more about this cosmic exploration [here](http://arxiv.org/abs/2503.04717v1).*\n\n## 🔧 Optimizing Hyperparameters in LLM Pretraining\n\nThe paper **Predictable Scale: Part I** identifies universal scaling laws for hyperparameters in language model pretraining.\n\n- **Objective**: Enhance model deployment efficiency through optimal hyperparameter scaling.\n- **Discovery**: Learning rates follow a power-law relationship with model and data sizes; batch sizes scale with data.\n- **Tool**: A plug-and-play estimator aids in finding optimal settings, aligning closely with global optima.\n\n*Check out the full research [here](http://arxiv.org/abs/2503.04715v1).*\n\n---\n\nThese groundbreaking studies continue to push the boundaries of what is possible in AI and robotics. Stay tuned for more insights and innovations in our next edition!"
          },
          "metadata": {}
        }
      ]
    }
  ]
}