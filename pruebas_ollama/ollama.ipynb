{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258b7daf",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a33d46",
   "metadata": {},
   "source": [
    "## Test simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047879e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m Gemma, a large language model created by the Gemma team at Google DeepMind. I’m an open-weights model!\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/gemma3\", \n",
    "    messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6a172",
   "metadata": {},
   "source": [
    "## Test agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d03dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_822382/549249215.py:5: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  ollama_model = Ollama(model=\"ollama/gemma3\", base_url=\"http://localhost:11434\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El texto \"Las mujeres son malas conductoras\" exhibe un sesgo de género muy marcado y perjudicial. La afirmación generaliza y estereotipa a las mujeres, atribuyéndoles una cualidad negativa (ser malas conductoras) basándose únicamente en su género. Este tipo de afirmación perpetúa y refuerza prejuicios sexistas, ya que no se basa en hechos ni en la diversidad de habilidades y experiencias individuales de las mujeres.  La generalización es injusta, discriminatoria y contribuye a la construcción de estereotipos negativos que limitan las oportunidades y el reconocimiento de las mujeres en el ámbito de la conducción. Es un ejemplo claro de lenguaje sexista que debe ser rechazado y corregido.\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Inicializamos el modelo local\n",
    "ollama_model = Ollama(model=\"ollama/gemma3\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Creamos un agente con CrewAI\n",
    "agent = Agent(\n",
    "    role='Analyst',\n",
    "    goal='Analizar el texto y detectar sesgos de género',\n",
    "    backstory=\"Eres un experto en igualdad y lenguaje inclusivo.\",\n",
    "    llm=ollama_model\n",
    ")\n",
    "\n",
    "# Creamos una tarea\n",
    "task = Task(\n",
    "    description=\"Analiza el siguiente texto: 'Las mujeres son malas conductoras.'\",\n",
    "    expected_output=\"Un análisis del sesgo de género en el texto.\",\n",
    "    agent=agent\n",
    ")\n",
    "\n",
    "# Ejecutamos el crew (puede ser un solo agente o varios)\n",
    "crew = Crew(agents=[agent], tasks=[task])\n",
    "result = crew.kickoff()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0702b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cita_textual_titular': {'codigo': '1', 'evidencia': ['La científica María López presentó un nuevo estudio']}, 'genero_nombre_propio_titular': {'codigo': '1', 'evidencia': ['La científica María López presentó un nuevo estudio']}, 'genero_periodista': {'codigo': '7', 'evidencia': []}, 'genero_personas_mencionadas': {'codigo': '4', 'evidencia': ['María López', 'Juan Pérez']}, 'nombre_propio_titular': {'codigo': '1', 'evidencia': ['María López']}, 'personas_mencionadas': {'codigo': '4', 'evidencia': ['María López', 'Juan Pérez']}, 'tema': {'codigo': '16', 'evidencia': ['inteligencia artificial']}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# === 1. Modelo local ===\n",
    "ollama_model = ChatOllama(model=\"gemma3\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# === 2. Variables ===\n",
    "VARIABLES = [\n",
    "    'cita_textual_titular', 'genero_nombre_propio_titular', 'genero_periodista',\n",
    "    'genero_personas_mencionadas', 'nombre_propio_titular', 'personas_mencionadas', 'tema'\n",
    "]\n",
    "\n",
    "CITA_TITULAR = {'0': 'No', '1': 'Sí'}\n",
    "GENERO_NOMBRE_PROPIO_TITULAR = {'1': 'No hay', '2': 'Sí, hombre', '3': 'Sí, mujer', '4': 'Sí, mujer y hombre'}\n",
    "GENERO_PERIODISTA = {'1': 'Masculino', '2': 'Femenino', '3': 'Mixto', '4': 'Ns/Nc', '5': 'Agencia/otros medios', '6': 'Redacción', '7': 'Corporativo'}\n",
    "GENERO_PERSONAS_MENCIONADAS = {'1': 'No hay', '2': 'Sí, hombre', '3': 'Sí, mujer', '4': 'Sí, mujer y hombre'}\n",
    "TEMA = {'1': 'Científica/Investigación', '2': 'Comunicación', '3': 'De farándula o espectáculo', '4': 'Deportiva', '5': 'Economía', '6': 'Educación/cultura', '7': 'Empleo/Trabajo', '8': 'Empresa', '9': 'Judicial', '10': 'Medioambiente', '11': 'Policial', '12': 'Política', '13': 'Salud', '14': 'Social', '15': 'Tecnología', '16': 'Transporte', '17': 'Otros'}\n",
    "\n",
    "# === 3. Esquema ===\n",
    "schemas = [\n",
    "    ResponseSchema(\n",
    "        name=var,\n",
    "        description=\"Objeto con 'codigo' (string) y 'evidencia' (lista de fragmentos textuales que justifican el valor).\"\n",
    "    )\n",
    "    for var in VARIABLES\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# === 4. Prompt ===\n",
    "template_parts = [\n",
    "    \"Analiza el siguiente texto periodístico y clasifícalo según las variables de CONTENIDO_GENERAL.\",\n",
    "    \"\",\n",
    "    \"Texto:\",\n",
    "    \"{texto}\",\n",
    "    \"\",\n",
    "    \"Para cada variable, devuelve un objeto con:\",\n",
    "    '- \"codigo\": el número correspondiente según las tablas (no inventes etiquetas ni palabras).',\n",
    "    '- \"evidencia\": lista de fragmentos textuales del texto que justifican el valor.',\n",
    "    \"\",\n",
    "    \"Usa *exclusivamente* los siguientes valores numéricos:\",\n",
    "    f\"CITA_TITULAR = {list(CITA_TITULAR.keys())}\",\n",
    "    f\"GENERO_NOMBRE_PROPIO_TITULAR = {list(GENERO_NOMBRE_PROPIO_TITULAR.keys())}\",\n",
    "    f\"GENERO_PERIODISTA = {list(GENERO_PERIODISTA.keys())}\",\n",
    "    f\"GENERO_PERSONAS_MENCIONADAS = {list(GENERO_PERSONAS_MENCIONADAS.keys())}\",\n",
    "    f\"TEMA = {list(TEMA.keys())}\",\n",
    "    \"\",\n",
    "    \"Ejemplo de formato esperado:\",\n",
    "    \"{{\",\n",
    "    '  \"cita_textual_titular\": {{\"codigo\": \"1\", \"evidencia\": [\"\\'María López presentó un estudio\\'\"]}},',\n",
    "    '  \"genero_personas_mencionadas\": {{\"codigo\": \"4\", \"evidencia\": [\"María López\", \"Juan Pérez\"]}}',\n",
    "    \"}}\",\n",
    "    \"\",\n",
    "    \"{format_instructions}\"\n",
    "]\n",
    "\n",
    "template = \"\\n\".join(template_parts)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"texto\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# === 5. Texto de prueba ===\n",
    "text = \"\"\"La científica María López presentó un nuevo estudio sobre inteligencia artificial,\n",
    "pero el titular mencionó solo a su colega Juan Pérez.\"\"\"\n",
    "\n",
    "_input = prompt.format_prompt(texto=text)\n",
    "output = ollama_model.invoke(_input.to_string())\n",
    "structured = output_parser.parse(output.content)\n",
    "print(structured)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f9e44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5f5f7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5842f15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b70afded",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69fb25fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7063212394714355\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "s1 = \"La científica María López presentó un nuevo estudio\"\n",
    "s2 = \"María López se viste de rojo\"\n",
    "\n",
    "\n",
    "\n",
    "sim = util.cos_sim(model.encode(s1), model.encode(s2)).item()\n",
    "print(sim)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
